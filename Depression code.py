# -*- coding: utf-8 -*-
"""Copy of Copy of projectwithawal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dB_9TgWoSSsXuwSdU2Ar3dpnQJWRkl_S
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.model_selection import train_test_split

from sklearn.svm import SVC

import datetime

start_time = datetime.datetime.now()
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/dataset/processData.csv')
#df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/depresion/processData1.csv')
#df = pd.read_csv('/content/drive/My Drive/dataset/depressionnew.csv')

X = df.iloc[:, [3,4,5,6,7,8,9]].values
y = df.iloc[:, 10].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=70)



end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Loading time in micro second: ", time_difference.microseconds)

df.head()



start_time = datetime.datetime.now()

start_time = datetime.datetime.now()
clf1 =  RandomForestClassifier(max_depth=5, random_state=41)
clf1.fit(X_train[:,[0,1,2,5,6]], y_train)

y_pred1 = clf1.predict(X_test[:,[0,1,2,5,6]])
print(y_pred1.shape)
print(y_test.shape)
print('test Accuracy :',metrics.accuracy_score(y_test,y_pred1))
print('Precision :',metrics.precision_score(y_test,y_pred1, average='macro'))
print('Recall :',metrics.recall_score(y_test,y_pred1, average='macro'))
print('F-score :',metrics.f1_score(y_test,y_pred1, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,y_pred1))
EPSILON = 1e-10
print('Mean Absolute Error(MAE):', metrics.mean_absolute_error(y_test, y_pred1))
print('Mean Squared Error(MSE):', metrics.mean_squared_error(y_test, y_pred1))
print('Root Mean Squared Error(RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))
print('Relative Absolute Error(RAE):', np.sum(np.abs(y_test - y_pred1)) / (np.sum(np.abs(y_test - np.mean(y_test))) + EPSILON))
print('Root Relative Squared Error(RRSE):', np.sqrt(np.sum(np.square(y_test - y_pred1)) / np.sum(np.square(y_test - np.mean(y_test)))))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Random Forest classifier time in micro second: ", time_difference.microseconds)

import numpy as np
from scipy import interp
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn.metrics import roc_curve, auc
import matplotlib.font_manager

plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']
# Plot linewidth.
lw = 2

n_classes=3
y_test=y_test
y_score=y_pred1
#y_test = np.array(test_generator)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure(1)
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])
# for i, color in zip(range(n_classes), colors):
#     plt.plot(fpr[i], tpr[i], color=color, lw=lw,
#              label='ROC curve of class {0} (area = {1:0.2f})'
#              ''.format(i, roc_auc[i]))

# plt.plot([0, 1], [0, 1], 'k--', lw=lw)
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.legend(loc="lower right")
# abc = 'vgg19-weighted-jaccard'
# plt.savefig(f'/content/drive/MyDrive/Machine Learning/iccv09 3 class/ROC/{abc}.png', bbox_inches ="tight", dpi = 520)
# plt.show()

start_time = datetime.datetime.now()

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
clf0 = BaggingClassifier(base_estimator=SVC(), n_estimators=5, random_state=5)
clf0.fit(X_train[:,[0,1,2,5,6]], y_train)

y_pred0 = clf0.predict(X_test[:,[0,1,2,5,6]])
print(y_pred0.shape)
print(y_test.shape)
print('test Accuracy :',metrics.accuracy_score(y_test,y_pred0))
print('Precision :',metrics.precision_score(y_test,y_pred0, average='macro'))
print('Recall :',metrics.recall_score(y_test,y_pred0, average='macro'))
print('F-score :',metrics.f1_score(y_test,y_pred0, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,y_pred0))
EPSILON = 1e-10
print('Mean Absolute Error(MAE):', metrics.mean_absolute_error(y_test, y_pred0))
print('Mean Squared Error(MSE):', metrics.mean_squared_error(y_test, y_pred0))
print('Root Mean Squared Error(RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred0)))
print('Relative Absolute Error(RAE):', np.sum(np.abs(y_test - y_pred0)) / (np.sum(np.abs(y_test - np.mean(y_test))) + EPSILON))
print('Root Relative Squared Error(RRSE):', np.sqrt(np.sum(np.square(y_test - y_pred0)) / np.sum(np.square(y_test - np.mean(y_test)))))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Bagging Classifier time in micro second: ", time_difference.microseconds)

"""decision tree"""

start_time = datetime.datetime.now()

clf = clf = DecisionTreeClassifier(random_state=45)
clf.fit(X_train[:,[0,1,2,5,6]], y_train)

y_pred = clf.predict(X_test[:,[0,1,2,5,6]])
print(y_pred.shape)
print(y_test.shape)
print('test Accuracy :',metrics.accuracy_score(y_test,y_pred))
print('Precision :',metrics.precision_score(y_test,y_pred, average='macro'))
print('Recall :',metrics.recall_score(y_test,y_pred, average='macro'))
print('F-score :',metrics.f1_score(y_test,y_pred, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,y_pred))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Decision Tree time in micro second: ", time_difference.microseconds)

"""random forest"""

start_time = datetime.datetime.now()


classifier = SVC(kernel = 'rbf', random_state = 0 )
classifier.fit(X_train[:,[0,1,2,5,6]], y_train)



y_pred =  classifier.predict(X_test[:,[0,1,2,5,6]])

print(y_pred.shape)
print(y_test.shape)
print('test Accuracy :',metrics.accuracy_score(y_test,y_pred))
print('Precision :',metrics.precision_score(y_test,y_pred, average='macro'))
print('Recall :',metrics.recall_score(y_test,y_pred, average='macro'))
print('F-score :',metrics.f1_score(y_test,y_pred, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,y_pred))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("SVC time in micro second: ", time_difference.microseconds)

"""Naive bayes"""

start_time = datetime.datetime.now()

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred2 = gnb.fit(X_train[:,[0,1,2,5,6]], y_train).predict(X_test[:,[0,1,2,5,6]])
print('test Accuracy :',metrics.accuracy_score(y_test,y_pred2))
print('Precision :',metrics.precision_score(y_test,y_pred2, average='macro'))
print('Recall :',metrics.recall_score(y_test,y_pred2, average='macro'))
print('F-score :',metrics.f1_score(y_test,y_pred2, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,y_pred2))
EPSILON = 1e-10
print('Mean Absolute Error(MAE):', metrics.mean_absolute_error(y_test, y_pred2))
print('Mean Squared Error(MSE):', metrics.mean_squared_error(y_test, y_pred2))
print('Root Mean Squared Error(RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))
print('Relative Absolute Error(RAE):', np.sum(np.abs(y_test - y_pred2)) / (np.sum(np.abs(y_test - np.mean(y_test))) + EPSILON))
print('Root Relative Squared Error(RRSE):', np.sqrt(np.sum(np.square(y_test - y_pred2)) / np.sum(np.square(y_test - np.mean(y_test)))))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Gaussion NB time in micro second: ", time_difference.microseconds)

"""naive bayes = 66% for 50

for 60
logistic = 67%
random=63%
decision = 60%

for 70
bagging =61%

logistic regression
"""

start_time = datetime.datetime.now()

from sklearn.linear_model import LogisticRegression
clf3 =  LogisticRegression(random_state=10)
clf3.fit(X_train[:,[0,1,2,5,6]], y_train)

y_pred3 = clf3.predict(X_test[:,[0,1,2,5,6]])
print(y_pred3.shape)
print(y_test.shape)
print('test Accuracy :',metrics.accuracy_score(y_test,y_pred3))
print('Precision :',metrics.precision_score(y_test,y_pred3, average='macro'))
print('Recall :',metrics.recall_score(y_test,y_pred3, average='macro'))
print('F-score :',metrics.f1_score(y_test,y_pred3, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,y_pred3))
EPSILON = 1e-10
print('Mean Absolute Error(MAE):', metrics.mean_absolute_error(y_test, y_pred3))
print('Mean Squared Error(MSE):', metrics.mean_squared_error(y_test, y_pred3))
print('Root Mean Squared Error(RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred3)))
print('Relative Absolute Error(RAE):', np.sum(np.abs(y_test - y_pred3)) / (np.sum(np.abs(y_test - np.mean(y_test))) + EPSILON))
print('Root Relative Squared Error(RRSE):', np.sqrt(np.sum(np.square(y_test - y_pred3)) / np.sum(np.square(y_test - np.mean(y_test)))))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Logistic Regression time in micro second: ", time_difference.microseconds)

start_time = datetime.datetime.now()

from sklearn.ensemble import VotingClassifier
eclf1 = VotingClassifier(estimators=[('rf', clf1), ('bg', clf0),('lr', clf3) ], voting='hard',weights=[3,2,1])
eclf1 = eclf1.fit(X_train[:,[0,1,2,5,6]], y_train)
final = eclf1.predict(X_test[:,[0,1,2,5,6]])
print('test Accuracy :',metrics.accuracy_score(y_test,final))
print('Precision :',metrics.precision_score(y_test,final, average='macro'))
print('Recall :',metrics.recall_score(y_test,final, average='macro'))
print('F-score :',metrics.f1_score(y_test,final, average='macro'))
print('Confusion Matrix:\n', metrics.confusion_matrix(y_test,final))
EPSILON = 1e-10
print('Mean Absolute Error(MAE):', metrics.mean_absolute_error(y_test, final))
print('Mean Squared Error(MSE):', metrics.mean_squared_error(y_test, final))
print('Root Mean Squared Error(RMSE):', np.sqrt(metrics.mean_squared_error(y_test, final)))
print('Relative Absolute Error(RAE):', np.sum(np.abs(y_test - final)) / (np.sum(np.abs(y_test - np.mean(y_test))) + EPSILON))
print('Root Relative Squared Error(RRSE):', np.sqrt(np.sum(np.square(y_test - final)) / np.sum(np.square(y_test - np.mean(y_test)))))


end_time = datetime.datetime.now()
time_difference = end_time - start_time
print("Voting Classifier time in micro second: ", time_difference.microseconds)
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

n_classes = 4   # the number of classes

# If your data is not one hotted then use this line of code
y_test = preprocessing.label_binarize(y_test, classes=range(n_classes))
y_score = preprocessing.label_binarize(final, classes=range(n_classes))


# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# Plot of a ROC curve for a specific class
plt.figure()
plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()

# Plot ROC curve
plt.figure()
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'
                                   ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Some extension of Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")
plt.show()

weighted_average =y_pred3+y_pred2+y_pred0+y_pred1
print('test Accuracy :',metrics.accuracy_score(y_test,weighted_average))

